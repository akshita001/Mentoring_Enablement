---
title: "predictWebTag"
author: "Kamal"
output: html_document
---
## R Markdown
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


### 1: Read data and Process


```{r, echo=FALSE}
#knitr::opts_chunk$set(echo = TRUE)

# Libraries Used
library(data.table)
library(ggplot2)
library(tm)
library(dplyr)
library(stringr)
library(tidytext)

data_path <- 'C:/Kamal/Work/19_AV/Innoplexus_Online_Hackathon/Data/'
clean = ''
list.files(data_path)

#dat_html_data <- read.csv(paste0(data_path, 'html_data',clean,'.csv'), stringsAsFactors = F)
dat_train <- read.csv(paste0(data_path, 'train',clean,'.csv'), stringsAsFactors = F)
table(dat_train$Tag)

dat_test <- read.csv(paste0(data_path, 'test',clean,'.csv'), stringsAsFactors = F)
#Split cells with commas in them
split2 <- strsplit(dat_test$Webpage_id.Domain.Url, split = "\\,")

# get the maximal length of the vectors in split2
ncol_new <- max(sapply(split2, length))
# set the length of all of those elements to the maximal lenght
# this also fills them with NA values
split3 <- lapply(split2, `length<-`, ncol_new)
# transform the list of vectors to a matrix
new_cols <- matrix(unlist(split3), byrow=TRUE, ncol=ncol_new)
# set new colnames
colnames(new_cols) <- paste0("newCol", 1:ncol_new)
# and add the matrix to the dataframe as new columns
df <- cbind(df, new_cols)
df <- as.data.frame(df)
dat_test <- df %>% select(newCol1,newCol2,newCol3) %>% rename(Webpage_id = newCol1,Domain = newCol2,Url = newCol3)
rm(df)

dat_test$Tag <- NA
dat_test$Tag <- as.character(dat_test$Tag)

print(sum(duplicated(dat_train$Webpage_id)))

```


```{r}
# Get the TRAIN dataset
dat_all <- dat_train
#rm(dat_train,dat_test)

# duplicate id values across categories, concat id to category and we're good
print(sum(duplicated(dat_all$Webpage_id)))

```

### 2: Declaring custom functions

```{r, echo=FALSE}
# removes all html tags
remove_html_tags <- function(htmlString) {
    return(gsub("<.*?>", "", htmlString))
}

# custom tokenizer
custom_tokenizer <- function(param_big_string) {
    #' lower-cases, removes punctuation, new line and return characters, 
    #' and removes unnecessary whitespace, then strsplits 
    split_content <- sapply(param_big_string, removePunctuation, preserve_intra_word_dashes=T)
    split_content <- sapply(split_content, function(y) gsub("[\r\n]", " ", y))
    split_content <- sapply(split_content, tolower)
    split_content <- sapply(split_content, function(y) gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", "", y, perl=TRUE))
    return(split_content)
    #return(split_content <- (sapply(split_content, strsplit, " ")))
}

```


### 3: Clean and Process data

```{r, echo=FALSE}
# quick clean on content column
dat_all$Url <- remove_html_tags(dat_all$Url)

# tokenize
dat_all$Domain <- custom_tokenizer(dat_all$Domain)
dat_all$Url <- custom_tokenizer(dat_all$Url)
dat_all$Tag <- custom_tokenizer(dat_all$Tag)

```


### 4: Text mining

```{r}
original_books <- data_frame(line = 1:length(dat_all$Url), text = dat_all$Url, category = dat_all$Tag)

stack_words <- original_books %>%
  unnest_tokens(word, text) %>%
  count(category, word, sort = TRUE) %>%
  ungroup()

stack_words <- stack_words %>%
  bind_tf_idf(word, category, n) 

stack_words

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
